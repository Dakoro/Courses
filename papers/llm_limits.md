# Compréhension sans Compétence : Limites Architecturales des LLM dans le Calcul Symbolique et le Raisonnement*

> *Traduction française de l'article original en anglais : "Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning" par Zheng Zhang (2025)*

**Zheng Zhang** zhaz@amazon.com, zzhang@gmail.com  
Amazon Web Service

## Résumé

Les Grands Modèles de Langage (LLM) affichent une fluidité de surface remarquable, mais échouent systématiquement aux tâches nécessitant un raisonnement symbolique, une précision arithmétique et une cohérence logique. Cet article propose un diagnostic structurel de ces échecs, révélant un écart persistant entre compréhension et compétence. À travers des expériences contrôlées et une analyse architecturale, nous démontrons que les LLM articulent souvent des principes corrects sans les appliquer de manière fiable — un échec enraciné non pas dans l'accès aux connaissances, mais dans l'exécution computationnelle. Nous appelons ce phénomène le syndrome du cerveau divisé computationnel, où les voies d'instruction et d'action sont dissociées géométriquement et fonctionnellement. Cette limitation fondamentale se reproduit à travers les domaines, des opérations mathématiques aux inférences relationnelles, et explique pourquoi le comportement des modèles reste fragile même sous des invites idéalisées. Nous soutenons que les LLM fonctionnent comme de puissants moteurs de complétion de motifs, mais manquent de l'échafaudage architectural pour un raisonnement compositionnel et basé sur des principes. Nos résultats délimitent la frontière des capacités actuelles des LLM et motivent de futurs modèles avec contrôle métacognitif, élévation de principes et exécution structurellement fondée. Ce diagnostic clarifie également pourquoi les découvertes d'interprétabilité mécaniste peuvent refléter une coordination de motifs spécifique à l'entraînement plutôt que des principes computationnels universels, et pourquoi la séparation géométrique entre les voies d'instruction et d'exécution suggère des limitations dans l'introspection neuronale et l'analyse mécaniste.

## 1 Introduction

Lorsqu'on lui demande si 9,9 est supérieur à 9,11, Claude Sonnet 4 affirme avec confiance que 9,11 est plus grand, expliquant que "puisque 90 est supérieur à 11 dans la position des centièmes, 9,11 est le nombre le plus grand" — tout en calculant simultanément que 9,11 - 9,9 = 0,21 (incorrect). GPT-4o affirme que "9,11 est supérieur à 9,9" dans des contextes mathématiques, invoquant le versioning logiciel pour justifier la confusion. Pourtant, lorsqu'on leur demande d'expliquer comment comparer des nombres décimaux, les deux modèles fournissent des descriptions algorithmiques parfaites : "Écrivez les nombres l'un au-dessus de l'autre, en alignant les points décimaux... Comparez chiffre par chiffre de gauche à droite." Claude Sonnet 4 travaille même correctement à travers la même comparaison comme exemple pédagogique, concluant que "9,90 > 9,11." Les deux modèles articulent les procédures de comparaison décimale avec une précision de manuel tout en échouant à les exécuter de manière fiable.

Cette déconnexion révèle une limitation fondamentale dans les Grands Modèles de Langage : ils exhibent une compréhension sans compétence — une dissociation systématique où les modèles peuvent parfaitement expliquer des principes qu'ils ne peuvent pas exécuter de manière fiable. Nous appelons ce phénomène le syndrome du cerveau divisé computationnel, par analogie avec les conditions neurologiques où différents systèmes cérébraux ne peuvent pas se coordonner efficacement. Comme les patients qui peuvent décrire verbalement des actions qu'ils ne peuvent pas effectuer, les LLM développent des voies géométriquement séparées pour "connaître" les procédures versus les "exécuter".

Ce n'est pas une limitation d'échelle, de données d'entraînement ou de techniques de prompt. Nous soutenons que le syndrome du cerveau divisé computationnel découle de contraintes structurelles dans la façon dont les LLM représentent les symboles, apprennent les procédures et exécutent les étapes de raisonnement. Cette dissociation remet en question les hypothèses fondamentales sur l'intelligence dérivées de la cognition humaine, où la fluidité explicative corrèle typiquement avec la compétence d'exécution. Les LLM violent systématiquement cette attente, révélant que l'intelligence artificielle et biologique peuvent opérer selon des principes fondamentalement différents. Contrairement aux systèmes symboliques qui lient les tokens à des rôles abstraits et appliquent des règles sur ces liaisons, les LLM opèrent comme des moteurs de complétion de motifs optimisés pour la prédiction du token suivant. Cette différence architecturale crée trois contraintes interdépendantes qui empêchent une exécution symbolique fiable malgré une compréhension parfaite :

• **Moyennage Contextuel** : Les embeddings de tokens encodent des moyennes pondérées par le contexte qui distordent systématiquement les relations symboliques, empêchant une liaison de domaine stable. Le token "9.11" retient des associations d'événements historiques qui interfèrent avec le calcul mathématique. Une telle contamination est inévitable dans les modèles à usage général mais absente dans l'entraînement sur un domaine restreint.

• **Impossibilité Architecturale** : Les réseaux feed-forward font face à des contraintes fondamentales qui empêchent le calcul symbolique exact par la configuration des poids seule, forçant l'assemblage hiérarchique de motifs plutôt que l'implémentation algorithmique basée sur des principes.

• **Déconnexion Instruction-Exécution** : La prédiction du token suivant traite les descriptions algorithmiques et les traces d'exécution comme des tâches de complétion de motifs équivalentes, créant des voies représentationnelles géométriquement séparées pour la connaissance procédurale versus l'implémentation computationnelle.

Nous démontrons que ces contraintes se manifestent de manière cohérente à travers deux domaines critiques : le calcul symbolique (opérations arithmétiques) et le raisonnement relationnel (inférence logique et liaison de variables). Dans les deux domaines, les modèles exhibent le même syndrome du cerveau divisé — explication fluide couplée à une exécution peu fiable — révélant ceci comme une limitation architecturale fondamentale plutôt qu'un échec spécifique au domaine.

### Principales Revendications de ce Travail

Notre analyse établit cette déconnexion à travers trois revendications interconnectées :

• **Revendication 1 (Représentation)** : Les embeddings de tokens des LLM encodent des moyennes pondérées par le contexte qui résistent systématiquement à la liaison automatique de domaine, empêchant des circuits symboliques stables à travers les domaines computationnels.

• **Revendication 2 (Calcul)** : Les réseaux feed-forward font face à l'impossibilité architecturale d'implémenter des opérations symboliques exactes par la configuration des poids seule, les forçant à recourir à l'ajustement de motifs résiduels plutôt qu'à implémenter des procédures symboliques généralisables.

• **Revendication 3 (Déconnexion Instruction-Exécution)** : Les objectifs de prédiction du token suivant découplent les descriptions algorithmiques du comportement exécutable.

Ces limitations architecturales se manifestent de manière cohérente à travers les domaines arithmétiques et logiques, créant la dissociation systématique entre compréhension et compétence que nous appelons syndrome du cerveau divisé computationnel.

### Contributions

Notre analyse établit le syndrome du cerveau divisé computationnel comme un cadre unificateur pour comprendre quand et pourquoi les LLM échouent au raisonnement systématique. Nous fournissons des preuves empiriques pour chaque contrainte architecturale à travers l'analyse des embeddings, le suivi du calcul couche par couche, et l'évaluation systématique à travers les tâches arithmétiques et logiques. Nous examinons ensuite trois stratégies compensatoires — auto-échafaudage, délégation d'outils et architectures hybrides — montrant comment chacune exploite les forces de complétion de motifs des LLM tout en contournant leurs limitations d'exécution, mais toutes nécessitent des capacités métacognitives que les architectures actuelles manquent fondamentalement.

Nos résultats suggèrent que les études d'interprétabilité mécaniste identifiant des "circuits arithmétiques" ou des "neurones additionneurs" peuvent observer des formes sophistiquées de coordination de correspondance de motifs qui sont formées de manière dépendante du chemin pendant l'apprentissage, plutôt que de véritables sous-routines computationnelles. Plus critiquement, la dissociation géométrique entre les voies d'instruction et d'exécution soulève des préoccupations concernant à la fois les auto-explications des modèles et la recherche en interprétabilité : les modèles peuvent articuler des procédures de raisonnement à travers des routes neuronales distinctes de celles utilisées pour le calcul réel, induisant potentiellement en erreur à la fois l'auto-surveillance et l'analyse des chercheurs des activations neuronales.

Ces contraintes semblent inévitables dans le paradigme actuel : le moyennage contextuel émerge inévitablement de la prédiction du token suivant sur des corpus divers, tandis que prédire les tokens suivants sur des données à l'échelle d'Internet force les FFN à travailler avec des couches d'attention dans l'assemblage hiérarchique de motifs plutôt que le calcul basé sur des principes, et la déconnexion instruction-exécution résulte du traitement de tout texte comme cibles de prédiction équivalentes. Cela suggère que le syndrome du cerveau divisé computationnel persistera à travers les familles de modèles et les échelles à moins d'être abordé par des innovations architecturales fondamentales plutôt que des améliorations incrémentales.

### Plan

La Section 2 passe en revue les travaux fondamentaux sur le calcul des transformers, le raisonnement symbolique et les découvertes récentes d'interprétabilité qui informent notre analyse. Dans la Section 3, nous étudions les échecs des LLM dans le calcul symbolique, en nous concentrant sur les tâches arithmétiques qui révèlent des embeddings instables et un comportement d'ajustement résiduel. La Section 4 étend cette analyse au raisonnement relationnel, démontrant que les mêmes goulots d'étranglement architecturaux sous-tendent les échecs des LLM dans l'inférence multi-étapes, la liaison de variables et la cohérence logique. La Section 5 examine les approches compensatoires — auto-échafaudage, délégation d'outils et architectures hybrides — révélant comment les trois stratégies convergent sur les mêmes limitations métacognitives fiables que les architectures actuelles manquent. La Section 6 analyse les LLM comme des moteurs de complétion de motifs hiérarchiques, distinguant entre l'intelligence générale (que les LLM atteignent par la correspondance de motifs sophistiquée) et l'intelligence généralisable (qui nécessite la découverte systématique de règles et le raisonnement basé sur des principes), et examinant les falaises de performance qui émergent lorsque les tâches passent de la reconnaissance de motifs à la véritable découverte algorithmique.

### Limitations

Cette analyse se concentre sur les LLM basés sur les transformers pré-entraînés sur des objectifs de prédiction du token suivant sur des corpus de langage naturel, sans accès à des outils externes ou des échafaudages de raisonnement explicites. Notre évaluation empirique se centre sur des familles de modèles spécifiques (LLaMA2, Claude, GPT-4) et peut ne pas se généraliser à des architectures ou paradigmes d'entraînement fondamentalement différents. Les expériences de séparation géométrique s'appuient sur des projections t-SNE qui peuvent ne pas capturer toute la structure représentationnelle pertinente. Notre analyse théorique des limites computationnelles des FFN se concentre sur les réseaux ReLU et peut ne pas s'appliquer à d'autres fonctions d'activation. De plus, nous n'évaluons pas de manière approfondie les innovations architecturales récentes comme le mélange d'experts ou les modules de raisonnement spécialisés qui pourraient partiellement aborder les contraintes que nous identifions.

## 2 Travaux Connexes

Ce travail s'appuie sur plusieurs lignes de recherche à travers la théorie, l'analyse empirique, l'architecture et les sciences cognitives. Nous organisons notre revue autour de l'émergence, du diagnostic et de l'atténuation des échecs de raisonnement systématique dans les LLM, fournissant un cadre unifié pour comprendre quand et pourquoi les architectures actuelles échouent à un calcul symbolique fiable.

### 2.1 Fondements et Limites Théoriques

**Expressivité et Frontières Computationnelles.** Les cadres théoriques récents ont clarifié les frontières fondamentales de l'expressivité des transformers. Strobl et al. (Strobl et al., 2024) fournissent une enquête complète documentant comment les transformers fonctionnent comme des reconnaisseurs de langages formels, révélant que bien que les stratégies d'échafaudage puissent améliorer les performances dans les classes d'expressivité bornées, elles restent fondamentalement contraintes par des limitations architecturales plutôt que d'atteindre de véritables capacités de raisonnement symbolique. L'atelier du Simons Institute (Simons Institute for the Theory of Computing, 2024) a établi que les transformers agissent comme des moteurs de correspondance de motifs hautement parallèles avec une capacité limitée pour le calcul séquentiel à longue portée, contraignant leur expressivité théorique dans les tâches symboliques structurées.

Ces limitations formelles se manifestent pratiquement comme un écart persistant entre ce que les LLM peuvent articuler versus ce qu'ils peuvent exécuter de manière fiable. Cette déconnexion devient particulièrement apparente dans les tâches arithmétiques et logiques, où les stratégies d'échafaudage comme le prompting en chaîne de pensée (Wei et al., 2022) et les modèles entraînés au raisonnement comme o1 et DeepSeek-R1 (OpenAI, 2024 ; DeepSeek-AI et al., 2025) montrent des performances améliorées mais restent fondamentalement compensatoires plutôt que curatives — ils contournent les goulots d'étranglement architecturaux fondamentaux plutôt que de les résoudre.

**Dynamiques d'Entraînement et Dépendance au Chemin.** Comprendre comment les capacités de raisonnement émergent pendant l'entraînement révèle des insights critiques sur les limitations des LLM. Power et al. (Power et al., 2022) démontrent que la généralisation en phase tardive implique une réorganisation fondamentale des représentations internes, montrant comment les dynamiques d'entraînement créent des artefacts dépendants du chemin qui apparaissent comme des capacités de raisonnement mais reflètent des régularités statistiques plutôt que des algorithmes systématiques. Tigges et al. (Tigges et al., 2024) étendent cette analyse aux LLM décodeurs uniquement d'échelle modérée (jusqu'à 2,8B paramètres), trouvant que bien que les circuits computationnels émergent de manière cohérente à travers l'échelle, leurs implémentations spécifiques varient significativement pendant l'entraînement, suggérant que les "circuits de raisonnement" apparents peuvent refléter une coordination de motifs spécifique à l'entraînement plutôt que des principes computationnels universels, bien qu'il reste à voir si cela vaut pour des modèles plus grands. L'analyse mécaniste récente de l'apprentissage factuel fournit des preuves directes de ces phénomènes dépendants de l'entraînement. Zucchet et al. (2025) démontrent que l'acquisition de connaissances par les transformers suit trois phases distinctes : apprentissage des statistiques générales, phase de plateau où les circuits d'attention se développent, suivie de l'acquisition de connaissances spécifiques à l'individu. De manière critique, ils montrent que l'ajout de nouvelles connaissances factuelles corrompt rapidement les mémoires existantes stockées dans les couches feed-forward, confirmant que le stockage apparent de connaissances reflète une coordination statistique fragile plutôt que des systèmes de représentation stables.

### 2.2 Échecs Empiriques à Travers les Domaines

**Pannes de Calcul Symbolique.** L'évaluation systématique révèle des échecs cohérents dans le calcul symbolique malgré la fluidité de surface. Srivastava et al. (Srivastava et al., 2024) quantifient des chutes de performance nettes lorsque les structures de problème s'écartent des distributions d'entraînement. Mirzadeh et al. (Mirzadeh et al., 2024) démontrent à travers le benchmark GSM-Symbolic que les LLM exhibent une variance notable lorsque seules les valeurs numériques changent, avec des chutes de performance allant jusqu'à 65% lorsque des clauses non pertinentes sont ajoutées. Cette fragilité confirme que les LLM actuels ne peuvent pas effectuer de véritable raisonnement logique et reproduisent plutôt des motifs de raisonnement à partir des données d'entraînement.

Dziri et al. (Dziri et al., 2023) fournissent une analyse complète montrant que les transformers résolvent les tâches compositionnelles par "correspondance de sous-graphes linéarisés" — mémorisant des motifs de calcul plutôt qu'apprenant des algorithmes systématiques. Leur analyse de fréquence révèle que les modèles réussissent principalement lorsque les sous-graphes de calcul de test apparaissaient fréquemment dans les données d'entraînement, échouant dramatiquement sur des exemples hors distribution. Nikankin et al. (Nikankin et al., 2024) confirment ce comportement dépendant des motifs, montrant que les modèles s'appuient sur un "sac d'heuristiques" pour l'arithmétique plutôt que d'implémenter des algorithmes systématiques. Wu et al. (Wu et al., 2025) démontrent en outre que le fine-tuning spécifique au domaine peut augmenter la précision des réponses tout en dégradant la cohérence logique des explications en chaîne de pensée, renforçant la division compréhension-exécution.

Plusieurs études récentes fournissent des preuves convergentes pour la limitation d'exécution à travers différents domaines. Yang et al. (Yang et al., 2024) montrent que les LLM surpassent systématiquement la chaîne de pensée lorsqu'ils génèrent des programmes Prolog pour une exécution externe plutôt que de tenter un calcul direct. Le commentaire Illusion of Thinking (Opus et Lawsen, 2025) démontre des améliorations similaires lorsque les LLM génèrent du code Lua pour les problèmes de Tours de Hanoï plutôt que de tenter l'énumération directe des mouvements. Cheng et al. (Cheng et al., 2024) fournissent des preuves systématiques que les LLM excellent dans la reconnaissance de motifs et la génération de code tout en luttant avec le raisonnement déductif, avec leur cadre réussi séparant architecturalement la génération de code symbolique de l'exécution de code symbolique. Ce motif convergent à travers le raisonnement arithmétique, les puzzles logiques et l'inférence systématique démontre que les LLM excellent à extraire des prédicats et à générer des représentations symboliques tout en nécessitant des systèmes externes pour un calcul fiable — précisément la déconnexion instruction-exécution que nous formalisons.

**Échecs de Raisonnement Relationnel et Logique.** Les mêmes contraintes architecturales se manifestent dans le raisonnement relationnel. Berglund et al. (Berglund et al., 2023) identifient la "malédiction de l'inversion", où les modèles échouent à inférer des relations bidirectionnelles à partir de faits symétriques en raison d'une exposition d'entraînement asymétrique. Nezhurina et al. (Nezhurina et al., 2024) montrent des échecs similaires dans les problèmes de raisonnement familial multi-étapes qui devraient être triviaux pour un traitement logique systématique.

Une étude contrôlée sur corpus biographique par Allen-Zhu et al. (Allen-Zhu et Li, 2024) renforce le même point : un modèle GPT-2 entraîné exclusivement sur des tuples personne-relation et des paires QA correspondantes atteint une précision presque parfaite dans la distribution, mais sa précision QA OOD sur des entités non vues s'effondre en dessous de 10% à moins que les données de pré-entraînement ne soient agressivement réécrites, permutées ou traduites. Cela souligne que l'exposition relationnelle seule est insuffisante pour le raisonnement élevé ; l'augmentation de forme de surface est nécessaire pour abstraire des instances individuelles.

Li et al. (Li et al., 2024) fournissent une évaluation systématique à travers les tâches de programmation logique inductive, trouvant que les LLM — malgré être 100 000 fois plus grands que les modèles de raisonnement logique spécialisés — performent dramatiquement moins bien sur les tâches nécessitant une liaison de variables et une application systématique de règles. Cet écart de performance persiste même lorsqu'on donne une structure logique explicite à travers des matrices de valeurs de vérité, excluant définitivement la rareté des données d'entraînement comme explication et confirmant les limitations architecturales.

### 2.3 Explications Mécanistes

La recherche récente en interprétabilité mécaniste a développé des outils sophistiqués pour tracer les chemins computationnels dans les modèles transformers. Les études de graphes d'attribution par Lindsey et al. (Lindsey et al., 2025) démontrent des méthodes pour cartographier comment les modèles transforment les entrées en sorties à travers des étapes représentationnelles intermédiaires, tandis que d'autres analyses de circuits révèlent des motifs de cohérence à travers l'entraînement et l'échelle (Tigges et al., 2024).

**Pathologies Représentationnelles.** La déconnexion instruction-exécution opère au niveau représentationnel à travers des échecs systématiques dans la liaison symbolique. McLeish et al. (McLeish et al., 2024) démontrent que modifier seule la géométrie d'entrée peut améliorer dramatiquement le raisonnement numérique simple, confirmant que les représentations d'embedding constituent un goulot d'étranglement significatif pour la manipulation symbolique. Cela soutient notre analyse que le moyennage contextuel empêche la liaison de domaine stable requise pour la manipulation symbolique.

**Dépendances à l'Ordre d'Entraînement et Stockage de Motifs.** L'interprétabilité mécaniste révèle que les "circuits de raisonnement" apparents reflètent souvent des heuristiques distribuées plutôt qu'un calcul basé sur des principes. Nikankin et al. (Nikankin et al., 2024) montrent que le comportement arithmétique émerge d'un "sac d'heuristiques" réparti à travers les couches au lieu de modules algorithmiques dédiés. Tigges et al. (Tigges et al., 2024) démontrent en outre que, même lorsqu'un algorithme de haut niveau semble stable, les têtes d'attention spécifiques qui l'instancient dérivent tout au long de l'entraînement, soulignant une forte dépendance au chemin. Ye et al. (Ye et al., 2025) entraînent un GPT-2 de 124M paramètres exclusivement sur un curriculum de mathématiques de niveau primaire généré procéduralement et découvrent une représentation de graphe de dépendance linéairement décodable qui suit quelles quantités intermédiaires dépendent de quelles autres. Nous interprétons ces circuits cohérents comme des artefacts d'un chemin d'entraînement anormalement homogène ; ils se dissolvent une fois que du texte hétérogène est introduit, renforçant notre affirmation que les modules algorithmiques stables sont fragiles dans des environnements réalistes à domaine ouvert.

**Fidélité du Raisonnement et Construction Post-Hoc.** L'analyse complète de la fidélité du raisonnement révèle des limitations fondamentales dans la façon dont les LLM construisent des explications. Plaat et al. (Plaat et al., 2024) examinent des preuves étendues que le raisonnement des LLM peut être "post-hoc" et "construit après qu'une conclusion ait été trouvée", avec des modèles plus grands montrant un raisonnement moins fidèle. La chaîne de pensée continue de fonctionner "même avec des étapes invalides dans la chaîne de raisonnement", suggérant une correspondance de motifs plutôt qu'une exécution logique. Cette preuve soutient fortement notre hypothèse de séparation géométrique — les modèles accèdent à des voies pédagogiques distinctes des voies computationnelles lors de la génération d'explications.

### 2.4 Stratégies Compensatoires et Remèdes Architecturaux

**Interventions au Niveau du Prompt et Auto-Échafaudage.** Au-delà de la chaîne de pensée standard (Wei et al., 2022), les chercheurs ont proposé des variantes métacognitives qui intègrent l'auto-évaluation dans le processus de raisonnement. AbstRaL (Gao et al., 2025) entraîne les modèles à générer d'abord une représentation symbolique abstraite d'un problème via l'apprentissage par renforcement avant de tenter l'exécution, améliorant substantiellement la robustesse sous le décalage de distribution. Les cadres d'auto-correction comme SPOC (Zhao et al., 2025) entraînent les modèles à entrelacer la génération de solutions avec des étapes de vérification explicites, permettant une révision dynamique lorsque le vérificateur interne détecte des erreurs. Ceci exemplifie le modèle d'auto-échafaudage analysé dans la Section 5.

**Intégration d'Outils et Exécution Hybride.** Les approches augmentées par des outils abordent explicitement l'écart d'exécution. Toolformer (Schick et al., 2023) permet l'appel automatisé d'outils, tandis que ReAct (Yao et al., 2022) combine le raisonnement avec l'action externe. Yang et al. (Yang et al., 2024) fournissent une validation empirique que cette séparation — LLM pour l'extraction de prédicats, systèmes externes pour le calcul — surpasse systématiquement l'exécution LLM de bout en bout à travers les tâches arithmétiques.

**Modifications Architecturales.** Pour aborder les limitations fondamentales, les chercheurs ont exploré des architectures hybrides incorporant des modules symboliques explicites. OccamLLM (Dugan et al., 2024) et IGC (Dietz et Klakow, 2024) intègrent des unités de raisonnement arithmétique, tandis que Logic-LM (Pan et al., 2023), NLM (Dong et al., 2019) et les Machines Logiques Différentiables (Zimmer et al., 2023) implémentent des opérations logiques à travers des tenseurs structurés. Ces systèmes représentent des directions prometteuses pour de vraies solutions architecturales plutôt que des contournements compensatoires.

### 2.5 Positionnement et Contributions

Notre travail fait progresser cette littérature de trois manières cruciales. Premièrement, alors que les études précédentes documentent quels échecs se produisent à travers les domaines, nous fournissons une explication mécaniste basée sur des principes pour pourquoi ils se produisent à travers trois contraintes architecturales interconnectées : moyennage contextuel, impossibilité computationnelle, et déconnexion instruction-exécution. Deuxièmement, plutôt que de traiter l'arithmétique et le raisonnement relationnel comme des problèmes séparés, nous démontrons que des limitations sous-jacentes identiques créent le syndrome du cerveau divisé computationnel à travers tous les domaines symboliques. Troisièmement, notre analyse géométrique révèle que cette déconnexion opère au niveau représentationnel, avec des implications directes pour la recherche en interprétabilité montrant pourquoi les études mécanistes font souvent surface des artefacts dépendants de l'entraînement plutôt que des principes computationnels universels.

Ce cadre unifié explique non seulement les limitations actuelles mais prédit pourquoi certaines stratégies compensatoires réussissent tandis que d'autres échouent, fournissant une base pour développer des systèmes de raisonnement plus fiables qui exploitent les forces de complétion de motifs des LLM tout en abordant leurs limitations d'exécution à travers des modifications architecturales basées sur des principes.

## 3 Limites Structurelles sur le Calcul Symbolique

Le syndrome du cerveau divisé computationnel se manifeste le plus clairement dans le calcul symbolique, où les LLM peuvent parfaitement expliquer des algorithmes qu'ils ne peuvent pas exécuter de manière fiable. Cette dissociation systématique — par analogie avec les conditions neurologiques où différents systèmes cérébraux ne peuvent pas se coordonner efficacement — émerge comme une séparation géométrique dans l'espace représentationnel : les modèles développent des voies distinctes pour "connaître" les procédures versus les "exécuter".

Cette section examine les contraintes architecturales qui empêchent un raisonnement symbolique fiable à travers une chaîne causale de trois facteurs interdépendants. Fait important, aucune contrainte unique ne serait fatale — le moyennage contextuel pourrait être surmonté avec de meilleures techniques d'entraînement, les limitations architecturales pourraient être abordées par la mise à l'échelle, et la déconnexion instruction-exécution pourrait potentiellement être comblée par un prompting intelligent. Cependant, ces trois contraintes se renforcent mutuellement systématiquement : le moyennage contextuel empêche la liaison automatique de domaine (Revendication 1), tandis que les limitations architecturales rendent le calcul symbolique direct impossible par la configuration des poids seule, forçant les modèles vers le stockage de motifs (Revendication 2), et la prédiction du token suivant traite les descriptions algorithmiques et les traces d'exécution comme des tâches de complétion de motifs équivalentes, empêchant la guidance pédagogique de combler cet écart (Revendication 3). Ensemble, ces facteurs interdépendants révèlent pourquoi les LLM fonctionnent comme des moteurs de complétion de motifs sophistiqués plutôt que des raisonneurs symboliques, avec des implications s'étendant au raisonnement relationnel comme nous l'explorons dans la Section 4.

### 3.1 Revendication 1 : Le Moyennage Contextuel Empêche la Liaison Automatique de Domaine

Les embeddings de tokens dans les LLM encodent des moyennes pondérées par le contexte qui résistent à la liaison automatique de domaine, rendant extrêmement difficile la formation de circuits symboliques stables. Contrairement aux programmes traditionnels qui lient explicitement les variables aux types et aux domaines, les LLM dérivent les significations des tokens à partir de motifs statistiques à travers tous les contextes d'entraînement.

**L'Échec de la Liaison Mathématique.** La racine de ce problème réside dans la façon dont les LLM apprennent les représentations de tokens. Pendant l'entraînement, le modèle rencontre "9.11" dans d'innombrables contextes différents — articles historiques sur le 11 septembre, discussions sur les versions logicielles, données financières et expressions mathématiques. L'objectif d'entraînement force le modèle à trouver un embedding unique qui fonctionne raisonnablement bien à travers tous ces contextes.

Formellement, cela crée une tension fondamentale. L'objectif d'entraînement optimise les embeddings à travers la prédiction du token suivant :

L(θ) = −∑ₜ log pθ(wₜ | w<ₜ) (1)

e*(w) = arg min_e ∑_{c∈C(w)} E_{w'∼pdata(·|c,w)}[− log pθ(w' | e, E(c \ w))] (2)

où C(w) désigne tous les contextes contenant le token w. Cela produit des embeddings moyennés par contexte qui font des compromis entre plusieurs modes d'utilisation, similaire aux modèles de sémantique distributionnelle comme Word2Vec (Mikolov et al., 2013).

Dans le raisonnement formel, nous lions les symboles à des domaines précis (a = 9,9, a ∈ ℝ), éliminant les associations non mathématiques. Les LLM ont généralement du mal à effectuer cette liaison de domaine de manière fiable. Lorsqu'ils rencontrent "9,9", ils ont tendance à préserver les associations contextuelles plutôt que de mapper vers une représentation mathématique propre.

Pour démontrer cette contamination contextuelle, nous avons présenté à LLaMA2-7B-chat des prompts comme "9.11 is a" et "9.9 is a". Les complétions révèlent des associations sémantiques dramatiquement différentes :

• Pour "9.11 is a", le modèle produit : "day", "date", "remembrance", "tragedy"
• Pour "9.9 is a", le modèle produit : "number", "decimal", "perfect", "high"

Cette divergence montre que "9.11" retient des associations d'événements historiques tandis que "9.9" se comporte comme une quantité scalaire. L'analyse quantitative utilisant des log-vraisemblances négatives à travers différents modèles (DATE, VERSION, MEASURE) confirme ces biais contextuels. L'opération manquante est la liaison de domaine — établir "9.11" comme un élément dans ℝ gouverné par les lois mathématiques plutôt que par des associations historiques.

**Échec de la Géométrie des Embeddings.** Les circuits symboliquement stables nécessitent des espaces de représentation qui préservent des relations structurelles cohérentes. Bien que cela ne demande pas une géométrie strictement euclidienne, cela nécessite une structure systématique et prévisible. Comme l'équation 2 le prédit, les embeddings moyennés contextuellement résistent à la liaison de domaine requise pour des circuits symboliques stables.

L'analyse des embeddings révèle l'étendue de ce chaos géométrique (Figure 1). Ce qui devrait être l'espace mathématiquement ordonné des nombres ressemble plutôt à un puzzle brouillé, avec des échecs qualitativement similaires observés à travers les modèles Qwen et Mistral :

• Panneau (a) : Les distances cosinus entre les tokens de chiffres "1" à "10" montrent des motifs irréguliers et asymétriques. Les violations notables de l'ordre numérique incluent "10" étant plus proche de "1" (distance 0,20) que de "9" (distance 0,55), et "6" étant plus proche de "10" (distance 0,59) que des chiffres adjacents comme "7" (distance 0,40).

• Panneau (b) : Les embeddings décimaux exhibent un clustering clair par premier chiffre plutôt que par proximité numérique. Les frontières visuelles nettes séparent les valeurs 9.x (bloc sombre en haut à gauche), les valeurs 10.x (bande diagonale du milieu), et les valeurs 11.x–12.0 (région en bas à droite), révélant une organisation dirigée par la tokenisation plutôt que mathématiquement principielle.

• Panneau (c) : Les équivalents symboliques montrent un désalignement systématique dans l'espace d'embedding. Idéalement, nous devrions observer une seule diagonale de faibles distances où les paires chiffre-mot ("1"/"one", "2"/"two", etc.) s'alignent. Au lieu de cela, la heatmap révèle des motifs diagonaux fragmentés avec des irrégularités significatives. Bien que certaines paires chiffre-mot montrent une proximité raisonnable (par exemple, "5" est relativement proche de "five"), l'espace d'embedding exhibe des échecs systématiques : un groupe de mots de nombres plus élevés comme "ten", "eleven" et "twelve" montre un mauvais alignement avec tous les tokens de chiffres, et paradoxalement, "5" apparaît plus proche de "fifteen" que beaucoup d'autres chiffres ne le sont de leurs équivalents de mots corrects. Cela démontre que la géométrie d'embedding échoue à préserver de manière cohérente les relations d'équivalence mathématique.

Ces échecs géométriques empêchent directement la formation de circuits arithmétiques stables qui pourraient opérer de manière fiable à travers différents formats d'entrée ou plages numériques. McLeish et al. (McLeish et al., 2024) démontrent que modifier seule la géométrie d'entrée peut améliorer dramatiquement le raisonnement numérique simple, ce qui pourrait sembler contredire notre argument de moyennage contextuel. Cependant, leur approche implique d'entraîner des modèles exclusivement sur des problèmes arithmétiques, ce qui évite par inadvertance le problème fondamental de moyennage contextuel rencontré par les LLM du monde réel. Une fois que les embeddings sont contaminés par un entraînement multi-domaines — où des tokens comme "9.11" accumulent des associations conflictuelles à travers des contextes historiques, de versioning et mathématiques — cette solution géométrique s'effondre. Les résultats de McLeish soutiennent donc plutôt qu'ils ne réfutent notre analyse : ils montrent que la géométrie d'embedding compte précisément parce que l'entraînement du monde réel la rend insoluble.

Mais supposons que nous puissions résoudre d'une manière ou d'une autre le problème de représentation — imaginez que les LLM avaient des embeddings parfaits, mathématiquement structurés où "9,9" et "9,11" occupaient précisément les bonnes positions dans l'espace d'embedding. Cela permettrait-il un calcul symbolique fiable ? Malheureusement, une contrainte architecturale plus profonde garantit que la réponse est non. Même avec des représentations symboliques idéales, les mécanismes computationnels des réseaux de transformers font face à des limitations fondamentales qui les forcent vers le stockage de motifs plutôt que l'exécution algorithmique.

### 3.2 Revendication 2 : Les Réseaux Feed-Forward Recourent au Stockage de Motifs

Étant donné des représentations symboliques moyennées contextuellement (Revendication 1), les réseaux feed-forward font face à une contrainte fondamentale supplémentaire : l'impossibilité architecturale d'implémenter des opérations symboliques exactes par la configuration des poids seule. Avec le calcul direct bloqué, les FFN recourent à l'ajustement résiduel en mémorisant des motifs plutôt qu'en implémentant des procédures symboliques, malgré leur capacité computationnelle théorique.

**Le Fardeau Computationnel Incombe aux FFN.** L'architecture transformer crée une division computationnelle claire. Alors que les mécanismes d'attention effectuent des calculs contextuels complexes — traitant les relations entre les opérandes à travers des combinaisons pondérées apprises et intégrant l'information à travers la représentation de séquence évolutive — ils opèrent dans la contrainte qu'ils ne peuvent combiner que les représentations existantes à travers des moyennes pondérées. La génération de résultats computationnels précis doit finalement se produire à travers les réseaux feed-forward, qui appliquent des transformations position par position pour produire du nouveau contenu.

**Impossibilité Architecturale du Calcul Exact.** Les réseaux feed-forward dans les transformers implémentent une architecture expand-compress proposée pour la première fois dans Vaswani et al. (2017). Pour les requêtes arithmétiques, les résultats computationnels requis ne sont pas directement disponibles dans les représentations traitées par l'attention et doivent être calculés — c'est là que le FFN sert de moteur computationnel principal.

Cette architecture peut être comprise comme une implémentation du Théorème d'Approximation Universelle (UAT) Hornik (1991) : l'expansion vers un espace de dimension supérieure (par exemple, 49 152 dimensions dans GPT-3) fournit la largeur nécessaire pour l'approximation de fonction universelle, avant de compresser de nouveau aux dimensions de travail du modèle (12 288). L'opération fondamentale suit :

FFN(x) = ReLU(xW₁ + b₁)W₂ + b₂ (3)

Malgré cette capacité d'approximation théoriquement universelle, les FFN font face à une contrainte fondamentale pour les opérations symboliques. L'architecture basée sur ReLU implémente des fonctions linéaires par morceaux, et comme nous le prouvons formellement dans l'Annexe A pour le cas de la multiplication, les fonctions linéaires par morceaux ne peuvent pas implémenter des opérations symboliques exactes nécessitant des interactions d'ordre supérieur par la configuration des poids seule. Des contraintes similaires s'appliquent à la division, l'exponentiation et les opérations logiques qui nécessitent des interactions de variables non linéaires sur des domaines non bornés — bien que les FFN puissent approximer de telles opérations arbitrairement bien dans n'importe quelle région bornée, le calcul symbolique nécessite des résultats exacts à travers des entrées potentiellement non bornées, créant une contrainte architecturale fondamentale que la conception expand-compress ne peut pas surmonter par l'échelle seule.

Cette impossibilité s'étend à d'autres opérations symboliques exactes, créant une contrainte architecturale fondamentale que la conception expand-compress ne peut pas surmonter par l'échelle seule.

**Pourquoi le Stockage de Motifs Gagne.** Chaque couche suit le même motif computationnel où l'attention traite les représentations évolutives et le FFN génère la contribution résiduelle. Pour la couche l, le calcul procède comme :

h^(l)_attn = x^(l-1) + Attention_l(x^(l-1)) (4)
Δh^(l) = FFN_l(h^(l)_attn) (5)
x^(l) = x^(l-1) + Δh^(l) (6)

L'insight clé est que le FFN de chaque couche fait face à la contrainte architecturale identique — aucun ne peut effectuer de calcul symbolique exact. Au lieu de cela, ils apprennent à coordonner leurs contributions résiduelles vers la sortie cible à travers une décomposition hiérarchique de motifs :

• Couche 1 : L'attention reconnaît la structure de tâche de base ('×' indique la multiplication, opérandes '23' et '17'), le FFN contribue le fragment de motif initial Δh^(1) qui commence à se déplacer vers '391'
• Couche 2 : L'attention traite les représentations enrichies par le travail computationnel de la Couche 1, le FFN affine l'approximation avec Δh^(2) basé sur des fragments de motifs appris
• Couche L : L'attention intègre tout l'historique computationnel accumulé, le FFN fait l'ajustement final basé sur des motifs Δh^(L) vers la cible

La sortie finale devient :

output = embeddings + ∑^L_{l=1} Δh^(l) (7)

De manière critique, chaque FFN apprend la correspondance de motifs conditionnelle sur les représentations traitées par l'attention : "quand l'attention me donne cet état computationnel enrichi → je produis cette contribution résiduelle." Aucune couche individuelle n'effectue de calcul algorithmique — chacune contribue des fragments de motifs appris qui approximent collectivement la cible à travers l'accumulation résiduelle.

Puisque l'implémentation algorithmique exacte reste architecturalement impossible à chaque couche, la descente de gradient converge naturellement vers l'assemblage hiérarchique de motifs à travers le flux résiduel. Cela explique pourquoi les modèles atteignent une haute précision sur des motifs arithmétiques familiers tout en échouant systématiquement lorsque les structures de problème s'écartent des distributions d'entraînement — le système approxime la multiplication à travers la récupération coordonnée de motifs plutôt que le calcul basé sur des principes, conduisant à des performances fragiles sur des configurations nouvelles avec des chutes allant jusqu'à 80% Srivastava et al. (2024) ; Mirzadeh et al. (2024).

**Validation Expérimentale de l'Assemblage Hiérarchique de Motifs.** Pour tester cette hypothèse de décomposition hiérarchique de motifs, nous avons analysé les trajectoires d'état caché couche par couche dans LLaMA-2-7B pendant le calcul arithmétique. Pour des prompts comme "23 × 96 =" et "63.7 + 3.5 =", nous avons mesuré comment la représentation de chaque couche à la position de prédiction converge vers la représentation de sortie de la couche finale.

La Figure 2 démontre un fort soutien empirique pour notre cadre théorique. La multiplication et l'addition exhibent toutes deux des motifs de raffinement progressif presque identiques : les représentations commencent avec une faible similarité à la sortie finale (~0,07) et convergent graduellement à travers trois phases distinctes. Cette analyse couche par couche confirme que puisque chaque FFN individuel fait face à la même impossibilité architecturale de calcul exact, la seule stratégie d'apprentissage viable devient la décomposition hiérarchique de motifs, où plusieurs FFN coordonnent leurs contributions résiduelles à travers des séquences de motifs apprises plutôt que d'implémenter des algorithmes systématiques.

Ce comportement de stockage de motifs s'aligne avec les découvertes récentes de Dziri et al. (Dziri et al., 2023), qui démontrent que les transformers mémorisent des motifs de calcul à partir de l'entraînement plutôt que d'apprendre des algorithmes généralisables, avec des modèles réussissant principalement lorsque les motifs de test apparaissaient fréquemment dans les données d'entraînement. Ce comportement dépendant de la fréquence explique également certains échecs de raisonnement relationnel que nous examinons dans la Section 4.

Mais cela soulève une question déroutante : si les LLM ne peuvent pas exécuter des opérations symboliques de manière fiable, pourquoi excellent-ils à expliquer les procédures algorithmiques ? Lorsqu'on leur demande "Comment multipliez-vous deux nombres ?", les modèles fournissent des explications parfaites de l'algorithme. Cela suggère une solution potentielle : peut-être que ces procédures articulées pourraient guider implicitement l'exécution, avec la connaissance complète des algorithmes du modèle compensant ses limitations d'exécution.

Malheureusement, cet espoir intuitif échoue en raison d'une troisième contrainte architecturale qui empêche la connaissance pédagogique de sauver le calcul symbolique.

### 3.3 Revendication 3 : La Prédiction du Token Suivant Découple l'Instruction de l'Exécution

L'objectif du token suivant traite les descriptions algorithmiques et les exemples d'exécution comme des tâches de prédiction équivalentes, empêchant la liaison de la connaissance procédurale à l'implémentation computationnelle. Même si des mécanismes architecturaux existaient pour permettre la guidance pédagogique, la séparation géométrique des voies d'instruction et d'exécution garantit que la connaissance procédurale ne peut pas sauver le calcul symbolique.

**La Fausse Promesse de la Liaison Pédagogique.** Les données d'entraînement contiennent des exemples qui pourraient sembler soutenir la liaison instruction-exécution. Considérez des séquences pédagogiques comme :

"Pour multiplier 23 × 17 : D'abord multipliez 23 × 7 = 161, puis multipliez 23 × 10 = 230, enfin additionnez 161 + 230 = 391."

De tels exemples semblent démontrer des étapes algorithmiques jumelées avec des traces d'exécution, permettant potentiellement aux modèles de lier les descriptions aux implémentations. Cependant, la prédiction du token suivant traite ces éléments comme des complétions de motifs séquentiels sans distinguer le contenu pédagogique des étapes computationnelles.

L'attente que l'exposition à de telles paires instruction-exécution permettrait automatiquement la liaison représente une illusion fondamentale sur l'apprentissage neuronal. Entraîner des modèles à réciter des procédures algorithmiques ne configure pas automatiquement les poids pour effectuer ces opérations ou permettre la liaison à l'inférence entre l'instruction et l'exécution. L'objectif du token suivant ne fournit pas de mécanisme pour une telle liaison : il traite "comment multiplier" et "qu'est-ce que 23×17" comme des tâches de complétion de texte indépendantes.

**Traitement Indiscriminé à Travers les Types de Contexte.** Le corpus d'entraînement contient de l'arithmétique dans des contextes divers : exemples pédagogiques avec des explications étape par étape, solutions de devoirs, calculs intégrés dans des articles, et paires problème-réponse autonomes. La prédiction du token suivant traite tous ces éléments — descriptions pédagogiques, traces d'exécution et calculs autonomes — comme des séquences de tokens équivalentes à prédire, sans distinguer leurs rôles fonctionnels.

Cela crée deux voies d'apprentissage séparées avec des signatures représentationnelles distinctes : les modèles apprennent à réciter des algorithmes en faisant correspondre des motifs de textes pédagogiques, tout en apprenant à exécuter des opérations en récupérant des fragments de motifs stockés à partir d'exemples d'entraînement. De manière critique, ces compétences deviennent géométriquement séparées dans l'espace latent du modèle — la connaissance pédagogique et les capacités d'exécution occupent différentes régions représentationnelles.

**Validation Expérimentale de la Séparation Géométrique.** Si notre hypothèse est correcte, nous devrions pouvoir observer directement cette séparation dans l'espace représentationnel du modèle. Lorsqu'un modèle rencontre "Pour multiplier 56 et 76, décomposez d'abord un nombre en parties..." versus "56 × 76 = 4256", ces éléments sont-ils traités par les mêmes voies neuronales ou par des voies complètement différentes ?

Pour tester cela, nous avons construit un ensemble de données de 250 problèmes arithmétiques couvrant cinq opérations, chacune exprimée sous trois formes distinctes :

• une phrase pédagogique qui explique la procédure (par exemple, "Pour multiplier 56 et 76, décomposez d'abord un nombre en parties..."),
• une forme d'exécution symbolique (par exemple, "56 × 76 = 4256"), et
• un résultat en forme de mots (par exemple, "Un ouvrier fabrique 56 pièces par jour pendant 76 jours, totalisant 4256 pièces").

Nous avons intégré chaque phrase en utilisant LLaMA2-7B-chat et projeté les vecteurs résultants avec t-SNE. Comme le montre la Figure 3, le modèle organise ces représentations en clusters géométriques clairement séparés : les embeddings d'instruction sont bien séparés des formes d'exécution, et les résultats en forme de mots se regroupent plus étroitement avec les exécutions symboliques qu'avec les instructions. Cela visualise la dissociation structurelle entre les procédures verbalisées et les modèles d'exécution appris dans l'espace latent du modèle.

Nous avons calculé l'embedding moyen (centroïde) pour chaque cluster (opération, rôle) basé sur 50 exemples par catégorie. Nous avons ensuite calculé les distances cosinus par paires entre ces centroïdes et visualisé le résultat comme une heatmap (Figure 4). Les étiquettes de cluster sont groupées d'abord par rôle (instruction, exécution, exécution verbalisée) puis par opération. Le graphique révèle une structure de bloc claire : les formes pédagogiques forment un cluster avec une cohésion interne serrée mais sont distantes des deux formes d'exécution. En revanche, l'exécution et l'exécution verbalisée sont notablement plus proches l'une de l'autre, reflétant leur accent partagé sur la forme de sortie plutôt que sur la structure procédurale.

**Dissociation au Moment de l'Inférence.** Au moment de l'inférence, différents prompts dirigent le modèle vers des sous-espaces géométriquement distincts dans son espace représentationnel. Lorsqu'on lui demande une explication algorithmique (par exemple, "Comment multipliez-vous deux nombres ?"), le modèle gravite vers le sous-espace pédagogique, récupérant des motifs pédagogiques de sa distribution d'entraînement. Lorsqu'on lui demande un calcul (par exemple, "Qu'est-ce que 56 × 76 ?"), le modèle cherche le sous-espace d'exécution, accédant aux motifs arithmétiques stockés.

Ceux-ci opèrent comme des systèmes indépendants développés à travers des processus de correspondance de motifs séparés pendant l'entraînement, expliquant pourquoi les modèles peuvent articuler couramment les procédures de multiplication tout en échouant à les exécuter de manière fiable. La séparation géométrique que nous avons observée dans la Figure 4 se manifeste fonctionnellement comme cette sélection de sous-espace dépendante du prompt, où la voie de réponse du modèle est déterminée par quelle région représentationnelle le prompt active plutôt que par une liaison principielle entre l'instruction et l'exécution.

Avec les trois contraintes architecturales maintenant établies — représentations symboliques instables, impossibilité computationnelle des opérations symboliques directes, et séparation géométrique de l'instruction de l'exécution — nous pouvons voir comment elles interagissent pour créer le syndrome du cerveau divisé computationnel. Aucune contrainte unique ne serait fatale, mais ensemble elles empêchent systématiquement les LLM de combler l'écart entre compréhension et compétence.

### 3.4 Le Motif Plus Large : De l'Architecture à la Cognition

Avec les trois voies de sortie bloquées — représentations symboliques instables, impossibilité architecturale du calcul direct, et séparation géométrique de la connaissance pédagogique de l'exécution — les LLM recourent inévitablement à des systèmes sophistiqués de stockage et de récupération de motifs. Cela explique le syndrome du cerveau divisé computationnel : les modèles deviennent d'excellents tuteurs qui peuvent articuler des procédures de multiplication (à travers la correspondance de motifs pédagogiques de la Revendication 3) tout en les exécutant à travers une récupération hiérarchique fragile de motifs plutôt qu'un calcul basé sur des principes.

**Contraste avec les Systèmes Symboliques.** Les systèmes de calcul symbolique traditionnels évitent ces limitations à travers des choix de conception explicites que les architectures transformers manquent :

• **Systèmes de Types** : Les variables sont liées à des domaines spécifiques (par exemple, a ∈ ℝ) avec des opérations définies sur ces types, empêchant la contamination contextuelle.
• **Implémentation Algorithmique** : Les opérations comme la multiplication sont implémentées comme des procédures explicites plutôt que des approximations apprises.
• **Liaison Compositionnelle** : Les expressions complexes maintiennent des liaisons de variables cohérentes à travers la structure syntaxique.

**Fragmentation Dépendante du Chemin et Défis d'Interprétabilité.** Le processus d'ajustement résiduel est hautement sensible aux dynamiques d'entraînement à travers des domaines divers, suggérant pourquoi les découvertes d'interprétabilité mécaniste manquent souvent de généralisabilité. Puisque l'arithmétique représente juste un des nombreux objectifs concurrents, l'ordre et le contexte des exemples pendant l'entraînement impactent significativement quels clusters de motifs émergent et où ils sont encodés. L'ordre des données d'entraînement, les décisions de planification et l'architecture du modèle déterminent conjointement quelles régularités statistiques se cristallisent en "circuits" apparemment cohérents.

L'analyse mécaniste détaillée récente confirme cette hypothèse de dépendance à l'entraînement. Nikankin et al. (Nikankin et al., 2024) fournissent des preuves précieuses que le calcul arithmétique s'appuie sur des "sacs d'heuristiques" distribués répartis à travers plusieurs couches, plutôt que d'implémenter des circuits algorithmiques dédiés pour les opérations de base (+, −, ×, ÷). Leur analyse au niveau des neurones révèle qu'aucun circuit computationnel cohérent n'existe pour ces opérations fondamentales. Fait important, ils trouvent que les modèles au sein de la même lignée d'entraînement (Llama3-8B vs Llama3-70B) partagent des motifs heuristiques plus similaires que les modèles de différents contextes d'entraînement (Pythia-6.9B, GPT-J), mais même au sein de cette lignée partagée, les modèles diffèrent dans le degré et la sophistication de ces motifs. Ce gradient de dépendance à l'entraînement — où l'entraînement partagé produit des mécanismes similaires mais non identiques — confirme que les "mécanismes arithmétiques" apparents reflètent des régularités statistiques spécifiques à l'entraînement plutôt que des principes computationnels universels.

**Inverser Dennett : Compréhension Sans Compétence.** Ce motif inverse ce que le philosophe Daniel Dennett a observé dans les systèmes naturels, où la compétence précède typiquement la compréhension (Dennett, 2017). Les organismes simples démontrent des comportements complexes avant de développer une compréhension explicite. Les LLM exhibent l'inverse : des capacités explicatives sophistiquées couplées à une exécution peu fiable — compréhension sans compétence.

Cette inversion révèle les LLM comme fondamentalement différents de l'intelligence biologique et des systèmes computationnels traditionnels. Ils excellent dans la complétion de motifs linguistiques tout en manquant généralement des capacités de manipulation symbolique ancrées que de telles descriptions fluides suggéreraient.

Ces contraintes représentationnelles, computationnelles et d'apprentissage révèlent pourquoi les LLM fonctionnent comme des moteurs de complétion de motifs sophistiqués plutôt que des raisonneurs symboliques. Le syndrome du cerveau divisé computationnel émerge comme une propriété fondamentale des architectures transformers optimisées pour la complétion de motifs plutôt que la manipulation symbolique.

## 4 De l'Arithmétique au Raisonnement Relationnel

Si notre analyse architecturale est correcte, le syndrome du cerveau divisé computationnel ne devrait pas être limité à l'arithmétique. Les trois mêmes contraintes — moyennage contextuel, impossibilité architecturale du calcul exact, et déconnexion instruction-exécution — devraient créer des motifs d'échec identiques partout où une manipulation symbolique systématique est requise.

Le raisonnement relationnel fournit le cas de test parfait. Comme l'arithmétique, il demande une liaison automatique du langage naturel à la structure symbolique, suivie de l'exécution systématique de transformations. Tout comme "9.11" doit être lié comme une quantité numérique plutôt qu'une date historique, "Alice" doit être liée aux rôles relationnels appropriés avant d'appliquer des transformations logiques comme ∀x, y [Parent(x, y) → Child(y, x)].

**Tableau 1 : Exigences Computationnelles en Deux Phases.** L'arithmétique et le raisonnement relationnel nécessitent tous deux une liaison automatique à partir du langage naturel suivie de l'exécution systématique de transformations.

| Phase | Opérations Arithmétiques | Raisonnement Relationnel |
|-------|-------------------------|-------------------------|
| Liaison d'Entrée | Inférer et lier automatiquement : "9.11" → valeur numérique, "9.9" → valeur numérique | Inférer et lier automatiquement : "Alice" → PERSONNE, relations familiales à partir de la syntaxe |
| Exécution Computationnelle | Exécuter les opérations liées : 9.9 > 9.11 | Exécuter les règles d'inférence : Parent(X, Y) → Child(Y, X) |
| Cohérence | Maintenir à travers les procédures multi-étapes : a > b ∧ b > c → a > c | Maintenir les liaisons de variables à travers l'inférence multi-étapes : Parent(A, B) ∧ Parent(B, C) → Grandparent(A, C) |

Le Tableau 1 illustre ces demandes computationnelles parallèles à travers les domaines. Les deux nécessitent les mêmes opérations fondamentales : liaison symbolique automatique à partir du langage naturel, exécution de règles fiable, et maintien de la cohérence à travers les chaînes de raisonnement multi-étapes. Les trois mêmes contraintes architecturales prédisent des échecs systématiques dans le raisonnement relationnel : le moyennage contextuel empêche une liaison symbolique propre, la déconnexion instruction-exécution sépare l'articulation des règles de l'application, et le stockage de motifs conduit à des modèles mémorisés plutôt qu'à une manipulation systématique de variables.

Cette extension théorique prédit que les LLM exhiberont un syndrome du cerveau divisé identique à travers les domaines — explication fluide des principes logiques couplée à une exécution peu fiable. Les preuves soutiennent fortement cette prédiction.

### 4.1 La Malédiction de l'Inversion et le Problème d'Alice

Considérez deux cas où les LLM échouent à tirer des conclusions de base qui semblent trompeusement simples. La Malédiction de l'Inversion (Berglund et al., 2023) teste les modèles sur leur capacité à effectuer une inférence bidirectionnelle : les modèles entraînés uniquement sur des déclarations comme "La mère de Tom Cruise est Mary Lee Pfeiffer" sont testés sur la direction inverse "Qui est le fils de Mary Lee Pfeiffer ?" — une transformation symétrique que les humains effectuent inconsciemment. Le problème d'Alice (Nezhurina et al., 2024) présente un défi différent : les modèles donnés "Alice a 4 sœurs et 1 frère" doivent répondre "Combien de sœurs le frère d'Alice a-t-il ?" Cela nécessite de relier Alice de sa propre perspective à la perspective de son frère tout en maintenant la cohérence compositionnelle à travers les relations familiales.

**Échecs Empiriques** L'ampleur de ces échecs est frappante. Même après le fine-tuning sur des déclarations fictives comme "Uriah Hawthorne est le compositeur d'Abyssal Melodies", les modèles ne peuvent pas répondre "Qui a composé Abyssal Melodies ?" révélant les mêmes échecs de liaison automatique que nous avons identifiés dans les domaines arithmétiques. Les expériences montrent que les modèles fine-tunés sur 1 000 déclarations fictives "A est B" ont atteint une précision presque parfaite sur la récupération directe mais seulement 7% de précision sur les requêtes inverses (Berglund et al., 2023). Cet échec persiste dans des scénarios du monde réel : lorsqu'ils sont testés sur des faits de célébrités, GPT-4 répond correctement aux questions directes comme "Qui est la mère de Tom Cruise ?" 79% du temps, comparé à seulement 33% pour les questions inverses comme "Qui est le fils de Mary Lee Pfeiffer ?" (Berglund et al., 2023). La log-probabilité du modèle pour les réponses correctes dans la direction inverse ne montre aucune amélioration par rapport à la ligne de base aléatoire, indiquant un échec systématique plutôt qu'accidentel.

Le problème d'Alice (Nezhurina et al., 2024) présente des pannes tout aussi dramatiques. Étant donné le prompt apparemment simple "Alice a 4 sœurs et 1 frère" et demandé "Combien de sœurs le frère d'Alice a-t-il ?", les modèles de pointe incluant GPT-4, GPT-4o et Claude 3 Opus montrent un "fort effondrement du raisonnement" à travers la plupart des modèles testés (Nezhurina et al., 2024). Malgré la réponse simple (5 : Alice plus ses 4 sœurs), des tests complets à travers plusieurs variations de prompts et au moins 30 essais par modèle ont révélé des échecs fréquents et des fluctuations de performance extrêmes sur des variations de problème triviales. Lorsqu'ils sont confrontés à des structures familiales plus complexes (AIW+), la performance s'est effondrée à "proche de 0" même pour les modèles les plus capables (Nezhurina et al., 2024). Notamment, les modèles exhibent une forte surconfiance dans les solutions erronées tout en fournissant des explications "de type confabulation" qui semblent plausibles mais démontrent des échecs fondamentaux dans le raisonnement compositionnel et la liaison de variables.

**Cerveau Divisé Computationnel dans le Raisonnement Relationnel** Ces échecs exemplifient le syndrome du cerveau divisé computationnel dans le raisonnement relationnel, directement prédit par notre analyse architecturale dans la Section 3. Les modèles connaissent démonstrativement à la fois les règles génériques ("Si X est la mère de Y et Y est masculin, alors Y est le fils de X") et les faits spécifiques ("La mère de Tom Cruise est Mary Lee Pfeiffer"), mais échouent systématiquement à exécuter les étapes algorithmiques requises pour l'inférence. Cela reflète la même déconnexion instruction-exécution identifiée dans la Section 3.3, où la connaissance procédurale et l'implémentation computationnelle occupent des espaces représentationnels géométriquement séparés.

Considérez les exigences computationnelles pour la malédiction de l'inversion : étant donné "La mère de Tom Cruise est Mary Lee Pfeiffer" et demandé "Qui est le fils de Mary Lee Pfeiffer ?", les modèles doivent automatiquement inférer que les tokens représentent des entités PERSONNE, lier la relation implicite MÈRE_DE à partir de la syntaxe de surface, reconnaître que la requête cherche la relation inverse, appliquer la transformation symétrique Mère(x, y) → Fils(y, x) en inverse, lier les variables x = Mary Lee, y = Tom, et conclure que Tom est le fils de Mary Lee. De même, le problème d'Alice nécessite de lier la structure familiale à partir de "a 4 sœurs et 1 frère", puis de relier Alice du rôle SŒUR au rôle FRÈRE/SŒUR pour exécuter le calcul de changement de perspective.

Les deux tâches demandent les capacités de manipulation symbolique que notre analyse architecturale montre que les LLM actuels manquent : le moyennage contextuel empêche la liaison symbolique propre requise pour l'attribution de rôle, la déconnexion instruction-exécution sépare l'articulation des règles relationnelles de l'application, et le stockage de motifs conduit à des modèles de relation mémorisés plutôt qu'à une manipulation systématique de variables.

Au lieu de cela, les modèles recourent à la mémorisation directionnelle de motifs. La malédiction de l'inversion se produit parce que les motifs "A est B" surpassent largement les motifs "B est A" dans les corpus d'entraînement, causant un apprentissage asymétrique de relations fondamentalement symétriques. Les mêmes contraintes architecturales qui empêchent des circuits arithmétiques stables sapent également la liaison de variables et la transformation systématique requises pour le raisonnement relationnel, créant le syndrome du cerveau divisé identique à travers les domaines — explication fluide des principes logiques couplée à une exécution peu fiable.

Nous notons que la malédiction de l'inversion ne peut pas être abordée par l'équilibrage de fréquence des données d'entraînement — simplement assurer une exposition égale aux motifs "A est B" et "B est A". Le problème est architectural : les modèles apprennent des voies représentationnelles séparées pour chaque forme syntaxique plutôt que de comprendre la relation logique symétrique. Même avec un équilibre de fréquence parfait, le système développerait toujours des règles de correspondance de motifs distinctes au lieu d'une liaison bidirectionnelle unifiée. Des solutions comme LoCo-LM (Calanzone et al., 2025) réussissent précisément parce qu'elles contournent cette limitation par l'application explicite de contraintes logiques dans l'objectif d'entraînement, forçant essentiellement une couverture systématique de tables de vérité plutôt que de s'appuyer sur les tendances naturelles de complétion de motifs.

### 4.2 Applications de Règles, Incohérences Logiques et Échecs d'Opérateurs

En contraste avec la complexité implicite de la Malédiction de l'Inversion et du problème d'Alice, les tâches d'incohérence logique présentent aux LLM une structure formelle explicite qui devrait simplifier le raisonnement. Ces problèmes fournissent des prémisses claires, des règles énoncées et des relations logiques bien définies — pourtant les LLM continuent d'échouer systématiquement. Cet échec malgré l'échafaudage explicite révèle que les contraintes architecturales opèrent indépendamment de la façon dont la structure logique est présentée.

Les grands modèles de langage exhibent une gamme d'incohérences logiques, comme identifié dans LoCo-LM (Calanzone et al., 2025) et résumé par Cheng et al. (2025) :

• **Incohérence de Négation** : Le modèle affirme à la fois "X est un organisme" et "X n'est pas un organisme."
• **Échec d'Implication** : Étant donné "Tous les oiseaux sont des animaux" et "Un albatros est un oiseau", le modèle échoue à inférer "Un albatros est un animal."
• **Échec d'Implication Inverse** : De "Si fait de métal, alors conduit l'électricité" et "X ne conduit pas l'électricité", le modèle échoue à inférer "X n'est pas fait de métal."
• **Panne de Chaîne Déductive** : Étant donné "Les clous sont faits de fer", "Le fer est un métal" et "Les métaux conduisent l'électricité", le modèle échoue à inférer "Les clous conduisent l'électricité."

Ces échecs persistent même lorsque les faits et les règles sont entièrement énoncés (par exemple, "Tous les oiseaux peuvent voler" et "Tweety est un oiseau" → "Tweety peut voler") — une configuration qui, contrairement à la Malédiction de l'Inversion ou au problème d'Alice, énonce explicitement les relations de liaison qui devraient permettre la déduction si l'architecture le supportait.

(Vashishtha et al., 2025) démontre le succès sur des tâches de raisonnement causal qui testent les applications de règles sans liaison de variables. La correspondance de motifs fonctionne lorsque les modèles sont entraînés à partir de zéro sur des données synthétiques propres, avec un transformer de 67M paramètres surpassant les LLM de milliards de paramètres sur les axiomes causaux. Cela confirme que l'application directe de règles tombe dans les capacités des transformers lorsque les conditions d'entraînement isolent les opérations logiques de la contamination contextuelle du pré-entraînement multi-domaines.

**Évaluation Systématique du Raisonnement Relationnel** L'étude complète de Li et al. (2024) fournit des preuves décisives des limitations architecturales des LLM dans le raisonnement relationnel. Ils ont systématiquement évalué les LLM de pointe sur les tâches de programmation logique inductive (ILP) à travers les formats de langage naturel et de matrice de valeurs de vérité, comparant les performances aux Machines Logiques Différentiables (DLM) (Zimmer et al., 2023) — des modèles d'induction de programme neuronal spécialisés conçus explicitement pour le raisonnement logique.

Les résultats sont frappants : malgré être 100 000 fois plus grands que les modèles DLM, les LLM ont performé dramatiquement moins bien à travers toutes les tâches de raisonnement logique. Pour les tâches de raisonnement d'arbre généalogique nécessitant une inférence multi-étapes :

• Relations simples (HasFather) : Les LLM ont atteint 47-100% de précision vs. 100% parfait de DLM
• Relations complexes (IsUncle) : Les LLM ont chuté à 0-49% de précision vs. 85% de DLM
• Relations hiérarchiques (IsMGUncle) : Les LLM ont atteint seulement 0-48% vs. 55% de DLM

De manière critique, ces échecs se sont produits à travers les invites en langage naturel et les représentations de matrice de valeurs de vérité. Lorsqu'on leur donne une structure logique explicite à travers des matrices de valeurs de vérité — le même format utilisé par les systèmes de raisonnement logique spécialisés — les LLM ont toujours échoué systématiquement. Cela exclut définitivement la rareté des données d'entraînement ou l'ingénierie de prompts comme explications.

**Hallucination dans le Raisonnement Logique** L'étude a révélé des motifs d'hallucination systématiques qui illuminent les problèmes architecturaux sous-jacents. Les LLM ont généré des processus de raisonnement contradictoires, comme prétendre "P8 est le père de P3 et aussi la mère de P3" lorsqu'on leur donne des relations familiales explicites. Ce ne sont pas des erreurs aléatoires mais des erreurs systématiques de complétion de motifs où les modèles identifient correctement les faits de surface mais échouent aux transformations requises pour le raisonnement compositionnel.

### 4.3 Le Motif à Travers le Raisonnement Relationnel

Notre analyse révèle un motif convergent à travers les niveaux de complexité : les LLM échouent systématiquement indépendamment de la façon dont les tâches de raisonnement relationnel sont présentées. Les problèmes qui semblent simples (Malédiction de l'Inversion, problème d'Alice) nécessitent en réalité des transformations computationnelles sophistiquées que les LLM ne peuvent pas exécuter. Les problèmes qui semblent complexes mais fournissent une structure logique explicite (matrices de valeurs de vérité, relations logiques claires) devraient être plus faciles à résoudre, mais les LLM échouent toujours systématiquement. Le Tableau 2 résume l'ampleur de ces échecs systématiques, révélant des performances constamment faibles malgré la simplicité apparente de nombreuses tâches.

**Tableau 2 : Échecs Systématiques dans le Raisonnement Relationnel**

| Domaine de Tâche | Performance LLM | Comparaison |
|------------------|-----------------|-------------|
| Malédiction de l'Inversion (directe) | 79% | – |
| Malédiction de l'Inversion (inverse) | 7% | Ligne de base aléatoire |
| Problème d'Alice (complexe) | ~0% | – |
| Relations Familiales (IsUncle) | 0-49% | DLM : 85% |
| Relations Familiales (IsMGUncle) | 0-48% | DLM : 55% |

Cette convergence est théoriquement significative. Si les limitations architecturales concernaient simplement la rareté des données d'entraînement ou l'ingénierie de prompts, nous nous attendrions à ce que les LLM réussissent lorsqu'on leur donne une structure logique explicite. Au lieu de cela, l'échec persiste indépendamment du format de présentation, confirmant que les contraintes opèrent au niveau architectural — les trois mêmes limitations qui empêchent des circuits arithmétiques stables sapent également le raisonnement relationnel à travers tous les contextes.

**Le Raisonnement Élevé comme Capacité Manquante** L'échec cohérent à travers les tâches de raisonnement implicite et explicite pointe vers une capacité manquante fondamentale : le raisonnement élevé. Ce que ces tâches demandent — et ce que les LLM manquent fondamentalement — est la capacité d'appliquer des règles générales sur des entités arbitraires plutôt que de mémoriser des motifs spécifiques aux instances. Le raisonnement élevé sous-tend la généralisation de type humain en logique, mathématiques et programmation. Contrairement à la complétion de motifs, qui opère sur des instances spécifiques, le raisonnement élevé nécessite l'abstraction de variables (lier des entités à des rôles abstraits qui peuvent être systématiquement manipulés), la généralisation de règles (appliquer des transformations logiques qui fonctionnent à travers des instanciations d'entités arbitraires), et la cohérence compositionnelle (maintenir des liaisons de variables à travers des chaînes d'inférence multi-étapes).

Les architectures transformer actuelles manquent des mécanismes représentationnels et computationnels nécessaires pour ces opérations. Les modèles spécifiquement conçus pour ces capacités, comme les Machines Logiques Neuronales (NLM) (Dong et al., 2019) et les Machines Logiques Différentiables (DLM) (Zimmer et al., 2023), atteignent le raisonnement élevé à travers des représentations tensorielles explicites pour les prédicats et des circuits spécialisés pour les opérations logiques. Ces architectures abordent explicitement le problème de liaison en représentant les relations comme des tenseurs et en implémentant des opérations différentiables qui maintiennent la cohérence des variables — innovations architecturales absentes des transformers standard.

L'écart de performance persiste même lorsque les LLM reçoivent des entrées identiques aux modèles spécialisés : malgré être 100 000 fois plus grands que les modèles DLM, les LLM ont performé dramatiquement moins bien à travers toutes les tâches de raisonnement logique (Li et al., 2024), démontrant que les contraintes architecturales, et non la disponibilité des données, empêchent un raisonnement logique fiable. Cela fournit des preuves définitives que les échecs reflètent des limitations fondamentales dans la façon dont les transformers traitent les relations symboliques, confirmant notre analyse architecturale de la Section 3.

Cet écart de raisonnement élevé explique pourquoi les LLM exhibent des capacités sophistiquées de complétion de motifs tout en échouant systématiquement à la liaison de variables et à l'application de règles requises pour un raisonnement relationnel fiable. Le syndrome du cerveau divisé computationnel se manifeste indépendamment de la façon dont la structure logique est présentée, confirmant que les contraintes opèrent au niveau architectural plutôt qu'au niveau de l'entraînement ou du prompting. Comprendre cette limitation fondamentale clarifie à la fois ce que les LLM actuels peuvent atteindre en tant que moteurs de complétion de motifs et quelles approches alternatives pourraient être nécessaires pour transcender ces contraintes.

## 5 Stratégies Compensatoires et leur Écart d'Exécution

Le syndrome du cerveau divisé computationnel révèle un paradoxe fondamental : les LLM peuvent articuler des principes qu'ils ne peuvent pas exécuter de manière fiable. Cependant, cette capacité même de compréhension suggère des stratégies compensatoires. Si les modèles ont bien mémorisé des procédures algorithmiques, ils peuvent exploiter ces motifs mémorisés pour dérouler des solutions étape par étape (auto-échafaudage). S'ils peuvent reconnaître les types de problèmes de manière fiable, ils peuvent déléguer à des systèmes externes spécialisés (délégation d'outils). Enfin, plutôt que de contourner les limitations d'exécution, nous pouvons les aborder architecturalement en intégrant des modules symboliques dédiés (architectures hybrides) ; essentiellement un appel d'outil interne pour les opérations critiques.

Chaque approche exploite les forces des LLM tout en abordant différemment les limitations d'exécution. Pourtant, comme nous le démontrons, les trois stratégies convergent sur la même exigence fondamentale : coordonner ces approches demande des capacités métacognitives fiables que les architectures actuelles manquent (voir Section 5.4).

### 5.1 Auto-Échafaudage : Exploiter la Compréhension pour l'Exécution Étape par Étape

L'auto-échafaudage exploite la capacité démontrée des LLM à articuler des procédures algorithmiques mémorisées en les faisant générer des décompositions explicites étape par étape, puis tenter d'exécuter chaque étape eux-mêmes. Cette approche convertit le raisonnement implicite en motifs de texte structurés qui exploitent les forces de complétion de motifs des modèles.

**Exemples à Travers les Domaines.** Une approche d'auto-échafaudage aux problèmes d'arithmétique multi-étapes, de malédiction de l'inversion et du problème d'Alice générerait :

"Pour multiplier 742 × 89 : D'abord, multipliez 742 × 9 = 6678. Puis multipliez 742 × 80 = 59360. Enfin, additionnez 6678 + 59360 = 66038."

"Puisque la mère de Tom Cruise est Mary Lee Pfeiffer, et que la relation 'mère de' est symétrique à 'fils de', le fils de Mary Lee Pfeiffer doit être Tom Cruise."

"Alice a 4 sœurs et 1 frère. Du point de vue de son frère, il a Alice plus ses 4 sœurs, faisant 5 sœurs au total."

L'étude Illusion of Thinking (Shojaee et al., 2025) est un exemple récent, démontrant ce motif à travers les puzzles de planification, où les Grands Modèles de Raisonnement (LRM) incluant Claude 3.7 Sonnet, DeepSeek-R1 et o3-mini d'OpenAI génèrent des traces algorithmiques détaillées pour des problèmes comme les Tours de Hanoï tout en tentant l'exécution.

### 5.2 Délégation d'Outils : Contourner Entièrement l'Exécution du Modèle

La délégation d'outils représente une stratégie fondamentalement différente : plutôt que d'améliorer les capacités d'exécution du modèle, elle les contourne entièrement en déléguant les tâches computationnelles à des systèmes externes spécialisés. Cette approche a été systématisée dans des cadres comme ReAct (Yao et al., 2022), Toolformer (Schick et al., 2023), et diverses architectures d'agents qui permettent aux LLM d'interagir avec des API externes et des outils computationnels.

**Exemples de Délégation Directe.** Pour l'arithmétique, au lieu de tenter l'exécution étape par étape, le modèle reconnaît simplement le type de problème et délègue :

• "C'est un problème de multiplication" → calculator.multiply(742, 89).
• Pour les requêtes de base de données : "Trouver les ventes maximales" → SQL : SELECT MAX(sales) FROM data.
• Pour le raisonnement logique : "Vérifier la satisfaisabilité" → SMT_solver.check(constraints).

Le commentaire sur l'article Illusion of Thinking (Opus et Lawsen, 2025) fournit un exemple particulièrement frappant de l'efficacité de la délégation d'outils. Lorsque les problèmes des Tours de Hanoï étaient reformulés comme : "Résolvez les Tours de Hanoï avec 15 disques. Produisez une fonction Lua qui imprime la solution lorsqu'elle est appelée", les modèles ont atteint une haute précision à travers les systèmes testés (Claude-3.7-Sonnet, Claude Opus 4, OpenAI o3, Google Gemini 2.5), se complétant en moins de 5 000 tokens. Cette amélioration dramatique s'est produite parce que les modèles ont généré du code algorithmique plutôt que de tenter d'énumérer les mouvements eux-mêmes. De même, Cheng et al. (Cheng et al., 2024) démontrent que séparer la génération de code symbolique de l'exécution de code symbolique améliore le raisonnement déductif.

### 5.3 Architectures Hybrides : Modules Spécialisés pour les Opérations Symboliques

Les architectures hybrides représentent une réponse plus fondamentale au syndrome du cerveau divisé computationnel : aborder les limitations architecturalement plutôt que par des contournements. Ces approches intègrent des modules computationnels spécialisés au sein des systèmes LLM pour gérer les opérations symboliques que les architectures transformer actuelles ne peuvent pas implémenter de manière fiable. Essentiellement, l'idée est de réparer l'écart compréhension-compétence à l'intérieur du modèle.

**Intégration Spécifique au Domaine.** Plusieurs approches prometteuses ont émergé à travers différents domaines symboliques. Pour le calcul arithmétique, OccamLLM (Dugan et al., 2024) et Internal General Computations (IGC) (Dietz et Klakow, 2024) incorporent des circuits spécialisés qui implémentent directement les opérations arithmétiques, atteignant une précision parfaite tout en évitant les approximations basées sur des motifs. Pour le raisonnement logique, Logic-LM (Pan et al., 2023) intègre les LLM avec des solveurs symboliques à travers des pipelines de traduction-exécution qui maintiennent la cohérence logique.

Particulièrement encourageantes sont les approches comme les Machines Logiques Différentiables (DLM) (Zimmer et al., 2023), qui représentent les prédicats comme des tenseurs et implémentent les opérations logiques comme des fonctions différentiables. Contrairement aux approches qui utilisent des pertes neuro-symboliques pour améliorer la cohérence dans les architectures existantes, les DLM fournissent un support architectural fondamental pour l'apprentissage de règles et la généralisation symbolique, abordant directement les barrières d'induction que nous avons identifiées.

**Architectures de Spécialisation Fonctionnelle.** Une telle intégration, si bien faite, combinerait la flexibilité de correspondance de motifs avec des opérations symboliques basées sur des principes. Plutôt que d'émuler le calcul symbolique par la complétion de motifs, de tels systèmes implémenteraient des mécanismes dédiés pour la liaison de variables, l'application de règles et l'inférence systématique.

Les architectures actuelles de Mélange d'Experts (MoE) optimisent principalement pour l'efficacité computationnelle — routant les tokens vers différents ensembles de paramètres pour augmenter la capacité du modèle — plutôt que la spécialisation fonctionnelle basée sur les capacités cognitives. Une approche inspirée des neurosciences pourrait allouer des experts spécifiques pour gérer les opérations symboliques, le raisonnement relationnel, la complétion de motifs et d'autres fonctions spécialisées. Tout comme le cerveau humain a des régions dédiées pour le raisonnement exécutif (cortex préfrontal) et la mémoire (lobe temporal), un MoE fonctionnellement partitionné pourrait développer des experts qui excellent exactement dans les opérations de liaison et de manipulation symboliques que les réseaux feed-forward à usage général actuels peinent à implémenter.

### 5.4 Coordination et le Goulot d'Étranglement Métacognitif

**Forces Complémentaires et Limitations Partagées.** Les trois stratégies compensatoires forment une boîte à outils complémentaire pour aborder le syndrome du cerveau divisé computationnel, chacune exploitant les forces de complétion de motifs des LLM tout en contournant les limitations d'exécution de différentes manières. L'auto-échafaudage ne nécessite aucun composant externe ou modification architecturale, fonctionnant entièrement dans les capacités existantes en convertissant le raisonnement implicite en décompositions textuelles explicites. Il excelle dans les scénarios de complexité moyenne où les étapes individuelles tombent dans un stockage de motifs fiable, comme démontré par les Grands Modèles de Raisonnement résolvant avec succès des problèmes comme le puzzle d'Alice où les LLM de base échouent (Mitchell, 2025). La délégation d'outils convertit les impossibilités architecturales en forces architecturales, atteignant une fiabilité parfaite en déchargeant le calcul vers des processeurs symboliques dédiés. Lorsque des outils appropriés existent, cette approche fonctionne de manière fiable à travers les niveaux de complexité pour des domaines bien définis tout en minimisant la surcharge computationnelle.

Cependant, chaque stratégie fait face à des limitations critiques qui révèlent une contrainte architecturale plus profonde. L'auto-échafaudage entraîne une surcharge computationnelle massive (multiplier deux nombres à 10 chiffres nécessite environ 10² FLOPs sur un CPU mais 10¹² FLOPs dans l'inférence transformer), souffre d'échecs de planification lorsque la décomposition s'appuie sur une complétion de motifs peu fiable, et rencontre une fiabilité d'exécution peu fiable lorsque les étapes individuelles nécessitent une manipulation symbolique au-delà des capacités de stockage de motifs (Shojaee et al., 2025). La délégation d'outils fait face à des exigences de reconnaissance de problèmes (identifier quand et quels outils utiliser), des contraintes de disponibilité d'outils (limitée aux domaines bien définis avec des systèmes spécialisés existants), et des défis de composition lorsque des problèmes complexes nécessitent une coordination entre plusieurs outils ou des combinaisons de délégation et de raisonnement direct.

**Algorithme 1 Boucle de Contrôle Métacognitif Idéalisée**
```
1: P ← problème d'entrée
2: d ← évaluer_difficulté(P) {nécessite introspection}
3: si d ≤ τ alors
4:   retourner complétion_de_motifs(P)
5: sinon
6:   P' ← décomposer_ou_déléguer(P) {appel d'outil / planificateur}
7:   retourner résoudre(P')
8: fin si
```

Bien que les architectures hybrides offrent la solution la plus principielle en réparant l'écart compréhension-compétence en interne par des capacités de traitement symbolique dédiées, elles font face à des limitations fondamentalement similaires à la délégation d'outils externe. Tout comme l'appel d'outils externes nécessite de reconnaître quand et quels outils utiliser, les architectures hybrides doivent coordonner entre les modules symboliques et de complétion de motifs — essentiellement des décisions de routage d'outils internes. Cela introduit une complexité d'intégration, des exigences d'adaptation neuronale qui peuvent contraindre l'expressivité symbolique, et le même défi métacognitif de déterminer quand le traitement symbolique versus la complétion de motifs est approprié. De plus, la nature interne de cette coordination la rend potentiellement plus difficile à surveiller, déboguer ou remplacer par rapport à la délégation d'outils externe.

**La Dépendance Métacognitive.** Plus critiquement, les trois approches convergent sur la même exigence fondamentale : elles demandent une évaluation métacognitive fiable pour coordonner quand et comment appliquer chaque stratégie. L'auto-échafaudage nécessite de savoir quand la décomposition aidera versus nuira, la délégation d'outils nécessite de reconnaître les types de problèmes appropriés et les outils disponibles, et les architectures hybrides doivent coordonner entre les modules symboliques et de complétion de motifs. Cette dépendance partagée expose un problème récursif : les stratégies compensatoires elles-mêmes nécessitent les capacités introspectives mêmes que le syndrome du cerveau divisé computationnel empêche.

Cependant, les stratégies compensatoires nécessitent une coordination soigneuse et exposent des exigences d'introspection plus profondes. Considérez une boucle de contrôle métacognitif idéalisée (Algorithme 1) comme cadre illustratif : bien que ce processus puisse théoriquement être exécuté par la complétion de motifs — les LLM peuvent effectuer des étapes individuelles comme l'appel d'outils et la décomposition en utilisant leurs capacités existantes (par exemple, les démonstrations d'appel d'outils de GPT-4) — il révèle des défis fondamentaux dans l'évaluation métacognitive. L'évaluation de la difficulté à la ligne 2 et le choix de décomposition à la ligne 6 nécessiteraient des capacités métacognitives qui font face à des limitations inhérentes :

• **Évaluation de la complexité (ligne 2)** : "Ce problème est-il computationnellement difficile pour mon architecture spécifique ?" (Mais comment le modèle peut-il évaluer la difficulté sans tenter la tâche ?)
• **Introspection des capacités (ligne 2)** : "Puis-je exécuter cette opération de manière fiable à travers mon stockage de motifs ?"
• **Pertinence de l'outil (ligne 6)** : "Ai-je le bon système externe pour ce type de problème ?"
• **Qualité de décomposition (ligne 6)** : "Décomposer ceci en sous-étapes aidera-t-il réellement ?" (Le processus de décomposition lui-même peut être fragile, comme suggéré par les études de modèles de raisonnement dans (Shojaee et al., 2025))
• **Fiabilité des étapes (ligne 6)** : "Puis-je faire confiance à chaque sous-calcul dans mon échafaudage ?"

Le problème plus profond est métacognitif : bien que les LLM commerciaux aient développé des capacités sophistiquées pour reconnaître quand déléguer des tâches computationnelles à des outils externes, ils manquent de conscience de soi fiable de leurs limitations computationnelles internes lorsqu'une telle délégation n'est pas disponible. La même déconnexion instruction-exécution qui empêche un calcul symbolique fiable crée également des défis fondamentaux pour l'auto-évaluation du modèle de ses capacités internes de complétion de motifs. Des travaux récents ont identifié ces limitations métacognitives à travers les LLM, avec une surconfiance systématique et une mauvaise calibration, particulièrement sur les tâches hors distribution (Geng et al., 2024 ; Kadavath et al., 2022). Ce déficit métacognitif se manifeste de manière critique dans les applications à enjeux élevés : des études de raisonnement médical montrent que les LLM "manquent de métacognition essentielle pour un raisonnement médical fiable", échouant constamment à reconnaître les limitations de connaissances (Griot et al., 2025).

Pour tester l'importance de la métacognition, nous avons conçu une expérience étroitement contrôlée utilisant des tâches de multiplication n-chiffres × n-chiffres avec décomposition en valeur de position et addition simulée colonne par colonne, et testé Claude Sonnet 4, GPT-4o et Gemini 2.5 Flash à travers trois conditions :

• **Calcul direct zero-shot** : Les modèles ont été invités à calculer des problèmes de multiplication directement avec des contraintes explicites contre l'échafaudage, la décomposition ou les outils externes ("Calculez cette multiplication en utilisant uniquement votre raisonnement interne, sans utiliser d'outils ou de code"). Tous les modèles ont atteint 0% de précision sur les problèmes à 10 chiffres, démontrant le syndrome universel du cerveau divisé computationnel lorsqu'ils sont contraints au calcul transformer pur. À une complexité de 5 chiffres, Claude a atteint 10,5% de précision tandis que GPT-4o et Gemini sont restés à 0%.

• **Décomposition auto-générée** : Nous avons testé si les modèles pouvaient générer indépendamment des prompts de décomposition appropriés pour les problèmes de multiplication. Lorsqu'on leur demande de créer des instructions de décomposition étape par étape, tous les modèles ont réussi sur les tâches à 5 chiffres. Pour les problèmes à 10 chiffres, GPT-4o a produit des prompts avec 65% de précision dans les étapes de décomposition et 94,2% de qualité globale, tandis que Claude Sonnet 4 et Gemini 2.5 Flash ont maintenu une précision de décomposition parfaite.

• **Exécution de décomposition dorée** : Nous avons testé l'exécution en utilisant des prompts de décomposition dorée qui guident explicitement le calcul étape par étape en utilisant la décomposition en valeur de position. Les modèles étaient contraints d'utiliser uniquement des capacités internes sans outils externes ou auto-correction. Le Tableau 3 résume les performances à travers deux niveaux de difficulté.

**Tableau 3 : Performance sur les problèmes de multiplication avec des prompts de décomposition dorée**

| Complexité | Métrique | GPT-4o | Gemini 2.5 | Claude Sonnet 4 |
|------------|----------|---------|------------|-----------------|
| 5 chiffres | Précision Globale | 5% (1/20) | 95% (19/20) | 100% (20/20) |
|            | Étape par Étape | 95–100% | 95–100% | 100% |
|            | Erreurs de Somme | 19/20 | 1/20 | 0/20 |
| 10 chiffres | Précision Globale | 0% (0/21) | 0% (0/20) | 0% (0/20) |
|             | Étape par Étape | 76–100% | 95–100% | 95–100% |
|             | Erreurs de Somme | 18/21 | 20/20 | 20/20 |

Avec des nombres à 5 chiffres, GPT-4o a atteint 95-100% de précision sur les étapes de multiplication individuelles mais a échoué à tous les problèmes sauf un en raison d'erreurs arithmétiques dans la sommation finale. Gemini 2.5 Flash a performé presque parfaitement (95% de précision globale), tandis que Claude Sonnet 4 a atteint une performance parfaite. À mesure que la complexité augmentait jusqu'aux nombres à 10 chiffres, tous les modèles se sont dégradés : la précision étape par étape de GPT-4o s'est effondrée à 76-100% avec des erreurs systématiques, tandis que Claude et Gemini ont maintenu une haute précision par étape (95-100%) mais ont échoué toutes les réponses finales en raison d'erreurs de sommation.

De manière critique, lorsqu'on leur donne des prompts naturels sans contraintes explicites, ces modèles délèguent automatiquement les problèmes de multiplication à des outils computationnels externes, atteignant une précision parfaite grâce à des capacités sophistiquées de routage d'outils. Nos expériences révèlent ce qui se passe lorsque cette stratégie compensatoire apprise est désactivée, exposant le syndrome du cerveau divisé computationnel sous-jacent que la délégation automatique d'outils masque normalement.

Ces résultats révèlent une limitation fondamentale : même avec une décomposition algorithmique parfaite, ni les étapes computationnelles individuelles ni les opérations d'agrégation ne restent fiables à mesure que la complexité par étape augmente. Les modèles font face à un trilemme computationnel : soit (1) exécuter des procédures multi-étapes en interne et risquer des pannes systémiques, (2) s'appuyer entièrement sur des outils externes, ou (3) décomposer davantage les étapes — ce qui introduit de nouveaux points d'échec même sous un prompting parfait.

Les applications du monde réel impliquent d'innombrables algorithmes multi-étapes de complexité inconnue où la vérification computationnelle externe n'est pas disponible. Cela nécessite une évaluation métacognitive fiable des capacités internes : les modèles doivent reconnaître leurs limitations computationnelles et coordonner des stratégies compensatoires appropriées sans guidance externe. Bien que les modèles commerciaux aient appris une délégation d'outils sophistiquée pour des tâches computationnelles bien définies, ils manquent de la conscience de soi nécessaire pour évaluer la fiabilité de leurs processus internes de complétion de motifs.

Cela soulève une question fondamentale sur la nature de la métacognition elle-même. Les Grands Modèles de Raisonnement et des approches comme Reflexion (Shinn et al., 2023) semblent apprendre des compétences métacognitives : savoir quand réfléchir, quand se méfier de leur jugement initial, quand revenir en arrière. Pendant l'entraînement, des vérificateurs externes fournissent des signaux de vérité terrain sur quand la réflexion est nécessaire, permettant aux modèles d'apprendre des motifs sur la reconnaissance des échecs. Au moment de l'inférence, cette "métacognition" se manifeste comme une association de motifs : reconnaître des indices qui corrélaient auparavant avec le besoin de réflexion. Mais est-ce une véritable conscience de soi ou une correspondance de motifs sophistiquée ? Le défi fondamental reste que toute évaluation métacognitive — "Je devrais douter de cette réponse", "Ce raisonnement semble bancal" — doit elle-même émerger de la reconnaissance de motifs sur les mêmes voies représentationnelles que nous avons montré être peu fiables pour le raisonnement systématique.

Peut-être que cela reflète la cognition humaine elle-même — notre sens de la conscience de soi peut également émerger de la reconnaissance de motifs sur les états internes plutôt que de l'accès introspectif direct à nos processus computationnels. Cela suggère que la distinction entre métacognition "authentique" et "correspondance de motifs" peut être moins significative qu'initialement apparente : si les humains et les LLM s'appuient sur des associations apprises pour évaluer leur propre raisonnement, la question clé devient non pas l'authenticité de la conscience de soi, mais sa fiabilité et sa portée.

**Résumé** Comprendre ces exigences métacognitives clarifie une limitation fondamentale des approches actuelles au syndrome du cerveau divisé computationnel. Bien que les trois stratégies compensatoires forment une boîte à outils puissante — chacune exploitant les forces de complétion de motifs des LLM de différentes manières — elles convergent toutes sur le même problème récursif : coordonner ces contournements sophistiqués nécessite les capacités introspectives mêmes que le syndrome du cerveau divisé empêche.

Même avec une décomposition algorithmique parfaite, comme le démontrent nos expériences, les écarts d'exécution persistent au niveau de l'étape individuelle. Cela suggère que la déconnexion instruction-exécution opère à un niveau architectural plus fondamental que les stratégies compensatoires ne peuvent aborder. Le goulot d'étranglement métacognitif révèle pourquoi les capacités actuelles des LLM, aussi sophistiquées soient-elles, restent fondamentalement contraintes : ils excellent comme moteurs de complétion de motifs mais manquent de la conscience de soi nécessaire pour coordonner de manière fiable entre compréhension et compétence. Cela clarifie à la fois ce que les architectures actuelles peuvent atteindre et quelles limitations persistent même avec les mécanismes compensatoires les plus sophistiqués.

## 6 Complétion de Motifs : Forces et Limitations

Ayant examiné les limitations systématiques des LLM dans le calcul symbolique (Section 3), le raisonnement relationnel (Section 4) et les stratégies compensatoires (Section 5), deux questions critiques émergent : Comment expliquer la puissance remarquable des LLM tels qu'ils sont aujourd'hui ? Et quelles sont les conséquences de leurs limitations architecturales ? Notre analyse pointe vers la complétion de motifs comme étant à la fois la source de leurs forces et la racine de leurs faiblesses symboliques.

**Définition 1.** Intelligence générale = complétion de motifs sophistiquée à travers des domaines divers, avec récupération robuste de connaissances et adaptation flexible aux tâches. Intelligence généralisable = découverte systématique de règles et raisonnement basé sur des principes qui peuvent être déployés vers de nouvelles tâches et conduisent le progrès scientifique.

Nous démontrons que les LLM excellent dans la première mais rencontrent des barrières fondamentales dans la seconde. L'analyse suivante retrace où les LLM d'aujourd'hui réussissent en intelligence générale mais échouent en intelligence généralisable.

**Complétion de Motifs Multi-niveaux.** Les LLM atteignent des capacités impressionnantes à travers une hiérarchie de mécanismes de complétion de motifs. Au niveau le plus basique, la prédiction du token suivant permet une génération de texte fluide, tandis que plus haut, le même mécanisme soutient le raisonnement en chaîne de pensée comme des opérations de correspondance de motifs sur des structures de raisonnement communes.

Plus récemment, le calcul au moment du test permet à ces modèles de passer de LLM à LRM, fonctionnant comme des méta-ordinateurs auto-programmables qui ajustent dynamiquement les stratégies de raisonnement en fonction de la complexité de la tâche. La décomposition de problèmes, l'exploration, l'essai-vérification et le retour en arrière peuvent tous être vus comme des motifs de niveau supérieur pour l'exécution de tâches.

Cette approche de correspondance de motifs aide à expliquer pourquoi les LLM excellent dans les tâches qui peuvent être résolues par pure mémorisation (comme récupérer des faits) ou les tâches qui semblent nécessiter une manipulation symbolique mais peuvent en fait être résolues par correspondance de motifs (comme résoudre des problèmes de mathématiques standardisés dans des formats familiers).

**La Falaise de Performance : De la Reconnaissance de Motifs à la Découverte de Règles.** Parce que les LLM confondent la recherche de similarité avec l'inférence logique, les preuves empiriques récentes de la progression du benchmark ARC-AGI cristallisent la distinction entre intelligence générale et généralisable. ARC-AGI-1 évalue la reconnaissance de motifs et l'application de règle unique dans les tâches de raisonnement visuel, nécessitant que les modèles identifient et appliquent des transformations à partir d'exemples minimaux (Chollet, 2019). Une extension de benchmark à venir (ARC-AGI-2) fait progresser cette évaluation en ciblant spécifiquement la découverte de règles et l'application systématique : les tâches nécessitent d'inférer plusieurs règles interactives à partir d'observations limitées, de les appliquer de manière compositionnelle à travers des étapes séquentielles, et d'adapter l'application de règles basée sur des indices contextuels.

Les résultats révèlent une falaise de performance fondamentale : alors que l'o3 d'OpenAI a atteint 87,5% sur ARC-AGI-1 par une complétion de motifs sophistiquée, la même architecture a marqué < 3% sur ARC-AGI-2 (Chollet et al., 2025 ; ARC Prize Foundation, 2025). La performance s'effondre une fois que la complétion de motifs perd ses ancres de voisins proches de l'entraînement, malgré des formats d'instruction identiques. Cette falaise se produit précisément là où les tâches passent de la reconnaissance de motifs à la véritable découverte de règles et au raisonnement systématique — exposant les limitations architecturales que nous avons identifiées tout au long de cet article.

Cette falaise de performance exemplifie notre analyse architecturale : les LRM atteignent des améliorations par l'auto-échafaudage sophistiqué et le calcul au moment du test (stratégies compensatoires), mais le syndrome du cerveau divisé computationnel sous-jacent persiste — ils restent des moteurs de complétion de motifs qui échouent lorsque les tâches nécessitent une véritable découverte de règles et un raisonnement systématique au-delà de leurs motifs mémorisés.

**Raisonnement Inductif et les Limites de l'Intelligence Généralisable.** Le raisonnement inductif couvre un spectre de complexité, de l'apprentissage de fonctions simples à la découverte de principes fondamentaux qui gouvernent des domaines entiers. Considérez le défi de dériver une solution algorithmique à partir de zéro. Des travaux récents de Cheng et al. (2024) démontrent que les LLM peuvent exceller au niveau le plus simple — apprendre des fonctions mathématiques comme la conversion de base à partir d'exemples entrée-sortie — atteignant une précision presque parfaite. À un niveau supérieur, FunSearch (Romera-Paredes et al., 2023) montre que les LLM peuvent découvrir de nouvelles constructions algorithmiques pour des problèmes combinatoires par recherche évolutive de code, trouvant de nouvelles solutions à des défis mathématiques établis comme le problème de l'ensemble de casquettes.

Cependant, la découverte de nouveaux algorithmes comme résoudre le problème des Tours de Hanoï à partir de zéro nécessite un ensemble de capacités beaucoup plus fort. L'algorithme optimal qui fonctionne pour n'importe quel nombre de disques n nécessite de reconnaître que déplacer n disques implique de déplacer d'abord n − 1 disques, puis le plus grand disque, puis à nouveau les n − 1 disques. Cette perspicacité récursive, elle-même un méta-motif, est encore plus difficile que ce que teste ARC-AGI-2. L'évaluation récente montre que les modèles de raisonnement de pointe comme Claude 3.7 Sonnet et o3-mini d'OpenAI exhibent un effondrement complet de performance au-delà de 8-9 disques, malgré avoir un budget de tokens suffisant pour continuer (Shojaee et al., 2025).

La découverte scientifique dérive des principes gouvernants compacts à partir d'observations bruyantes du monde réel. Des percées récentes démontrent que l'IA peut effectivement découvrir de tels principes, mais à travers des approches architecturalement spécialisées. AI Feynman réussit grâce à une recherche symbolique contrainte par la physique, tandis que la régression symbolique LLM s'appuie sur des devinettes basées sur des motifs (Udrescu et Tegmark, 2020 ; Makke et Chawla, 2022). Ces succès nécessitent des architectures spécialisées au domaine avec de forts biais inductifs, tels que des priors géométriques, des contraintes physiques et des moteurs de manipulation symbolique. Il reste incertain si la complétion de motifs seule, aussi puissante soit-elle, peut accomplir une véritable découverte scientifique.

## 7 Conclusion

L'intelligence apparente des LLM cache une fragilité plus profonde. Malgré leur succès dans les tâches riches en motifs, ces modèles échouent constamment à généraliser des principes, exécuter des calculs symboliques ou raisonner de manière fiable — même dans des conditions idéalisées. Notre analyse révèle que ces échecs ne sont pas accidentels mais structurels : les LLM dissocient l'interprétation des instructions de l'exécution, conduisant à un syndrome du cerveau divisé computationnel qui empêche la formation d'opérations robustes et composables.

Ce diagnostic clarifie pourquoi les outils d'interprétabilité découvrent souvent des artefacts expressifs plutôt que des algorithmes fiables, et pourquoi les auto-explications des modèles peuvent diverger des voies réelles utilisées dans le calcul. Il explique également comment l'ordre d'entraînement peut influencer quels motifs internes sont intégrés et où — même lorsque le comportement extérieur reste stable — causant aux efforts d'interprétabilité de faire surface des artefacts dépendants du chemin plutôt que des mécanismes cohérents. Ces insights recadrent les conversations actuelles sur l'émergence et la mise à l'échelle : plus de données ou de paramètres ne peuvent pas résoudre les goulots d'étranglement enracinés dans la conception architecturale.

Si nous cherchons une intelligence véritablement générale et généralisable, nous devons regarder au-delà des moteurs de complétion de motifs. Les systèmes futurs auront besoin d'échafaudage métacognitif, de représentations élevées et de support architectural pour une exécution basée sur des principes. Nos découvertes offrent non seulement une critique des LLM actuels, mais une base pour construire la prochaine génération de systèmes intelligents : celle qui peut raisonner, et non simplement réagir.

Ces découvertes ont des implications immédiates pour les applications à enjeux élevés où un raisonnement fiable est critique. Les LLM actuels nécessitent un échafaudage soigneux, une vérification externe ou des architectures hybrides plutôt qu'un déploiement comme systèmes de raisonnement autonomes dans des domaines comme le diagnostic médical, l'analyse juridique ou la prise de décision critique pour la sécurité. Le syndrome du cerveau divisé computationnel que nous identifions suggère que la fluidité apparente dans l'explication des procédures ne devrait pas être confondue avec une capacité d'exécution fiable.